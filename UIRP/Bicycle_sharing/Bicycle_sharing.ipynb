{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# (\"./data/etl_ymd.csv\")# 주말인지 아닌지 \n",
    "# (\"./data/\"id_WGS.csv) #위도 경도 데이터\n",
    "dir_=\"C://Users/sunkeun_jo/Desktop\\data/etl_ymd.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etl_ymd</th>\n",
       "      <th>etl_ymd_dateType</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180101</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180102</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180103</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180104</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180105</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    etl_ymd etl_ymd_dateType  weekday\n",
       "0  20180101       2018-01-01        0\n",
       "1  20180102       2018-01-02        1\n",
       "2  20180103       2018-01-03        2\n",
       "3  20180104       2018-01-04        3\n",
       "4  20180105       2018-01-05        4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(dir_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(df['etl_ymd'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etl_ymd</th>\n",
       "      <th>etl_ymd_dateType</th>\n",
       "      <th>weekday</th>\n",
       "      <th>etl_ymd_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180101</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180102</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180103</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180104</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180105</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    etl_ymd etl_ymd_dateType  weekday etl_ymd_Type\n",
       "0  20180101       2018-01-01        0   2018-01-01\n",
       "1  20180102       2018-01-02        1   2018-01-02\n",
       "2  20180103       2018-01-03        2   2018-01-03\n",
       "3  20180104       2018-01-04        3   2018-01-04\n",
       "4  20180105       2018-01-05        4   2018-01-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "df['etl_ymd_Type']=df['etl_ymd'].apply(lambda x : str(x)[0:4]+\"-\"+str(x)[4:6]+\"-\"+str(x)[6:])\n",
    "df['etl_ymd_dateType']=df['etl_ymd_Type'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etl_ymd</th>\n",
       "      <th>etl_ymd_dateType</th>\n",
       "      <th>weekday</th>\n",
       "      <th>etl_ymd_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>20181227</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>20181228</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>20181229</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>20181230</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>20181231</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      etl_ymd etl_ymd_dateType  weekday etl_ymd_Type\n",
       "360  20181227       2018-12-27        3   2018-12-27\n",
       "361  20181228       2018-12-28        4   2018-12-28\n",
       "362  20181229       2018-12-29        5   2018-12-29\n",
       "363  20181230       2018-12-30        6   2018-12-30\n",
       "364  20181231       2018-12-31        0   2018-12-31"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 공공 데이터포털\n",
    "\n",
    "공공데이터 포털에서 기상청18_동네예보_조회서비스_오픈API 서비스 사용 그 중 초단기 실황 조회로 그 당시의 실제 날씨를 찾고자함 \n",
    "\n",
    "울산광역시 중구 북정동에 위치한 관측소를 기준으로 여러가지 정보들을 추출했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "servicekey=\"0dK1EIo6UluKLHUFpdCi0cbu2M1yQ0xDrrUwxEoEy%2Fz3ZXptbQ6rQ7KqBmuj1mTyXKglkIhWyCjs8abFAYYdDg%3D%3D\"\n",
    "endpoint=\"http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList?serviceKey={}&numOfRows=10&pageNo=1&dataCd=ASOS&dateCd=HR&stnIds=152&schListCnt=10&endDt=20181231&endHh=23&startHh=01&startDt=20180101\".format(servicekey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList?serviceKey=0dK1EIo6UluKLHUFpdCi0cbu2M1yQ0xDrrUwxEoEy%2Fz3ZXptbQ6rQ7KqBmuj1mTyXKglkIhWyCjs8abFAYYdDg%3D%3D&numOfRows=10&pageNo=1&dataCd=ASOS&dateCd=HR&stnIds=152&schListCnt=10&endDt=20181231&endHh=01&startHh=01&startDt=20180101\n"
     ]
    }
   ],
   "source": [
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<response><header><resultCode>00</resultCode><resultMsg>NORMAL_SERVICE</resultMsg></header><body><dataType>XML</dataType><items /><numOfRows>10</numOfRows><pageNo>2911</pageNo><totalCount>8737</totalCount></body></response>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp=requests.get(endpoint)\n",
    "print(resp.status_code)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-30 19:00\n",
      " 데이터 1개가 들어갔습니다.\n",
      "2018-12-30 20:00\n",
      " 데이터 2개가 들어갔습니다.\n",
      "2018-12-30 21:00\n",
      " 데이터 3개가 들어갔습니다.\n",
      "2018-12-30 22:00\n",
      " 데이터 4개가 들어갔습니다.\n",
      "2018-12-30 23:00\n",
      " 데이터 5개가 들어갔습니다.\n",
      "2018-12-31 00:00\n",
      " 데이터 6개가 들어갔습니다.\n",
      "2018-12-31 01:00\n",
      " 데이터 7개가 들어갔습니다.\n",
      "2018-12-31 02:00\n",
      " 데이터 8개가 들어갔습니다.\n",
      "2018-12-31 03:00\n",
      " 데이터 9개가 들어갔습니다.\n",
      "2018-12-31 04:00\n",
      " 데이터 10개가 들어갔습니다.\n",
      "874번째 페이지의 내용이 들어갔습니다.\n",
      "2018-12-31 05:00\n",
      " 데이터 11개가 들어갔습니다.\n",
      "2018-12-31 06:00\n",
      " 데이터 12개가 들어갔습니다.\n",
      "2018-12-31 07:00\n",
      " 데이터 13개가 들어갔습니다.\n",
      "2018-12-31 08:00\n",
      " 데이터 14개가 들어갔습니다.\n",
      "2018-12-31 09:00\n",
      " 데이터 15개가 들어갔습니다.\n",
      "2018-12-31 10:00\n",
      " 데이터 16개가 들어갔습니다.\n",
      "2018-12-31 11:00\n",
      " 데이터 17개가 들어갔습니다.\n",
      "2018-12-31 12:00\n",
      " 데이터 18개가 들어갔습니다.\n",
      "2018-12-31 13:00\n",
      " 데이터 19개가 들어갔습니다.\n",
      "2018-12-31 14:00\n",
      " 데이터 20개가 들어갔습니다.\n",
      "875번째 페이지의 내용이 들어갔습니다.\n",
      "2018-12-31 15:00\n",
      " 데이터 21개가 들어갔습니다.\n",
      "2018-12-31 16:00\n",
      " 데이터 22개가 들어갔습니다.\n",
      "2018-12-31 17:00\n",
      " 데이터 23개가 들어갔습니다.\n",
      "2018-12-31 18:00\n",
      " 데이터 24개가 들어갔습니다.\n",
      "2018-12-31 19:00\n",
      " 데이터 25개가 들어갔습니다.\n",
      "2018-12-31 20:00\n",
      " 데이터 26개가 들어갔습니다.\n",
      "2018-12-31 21:00\n",
      " 데이터 27개가 들어갔습니다.\n",
      "2018-12-31 22:00\n",
      " 데이터 28개가 들어갔습니다.\n",
      "2018-12-31 23:00\n",
      " 데이터 29개가 들어갔습니다.\n",
      "876번째 페이지의 내용이 들어갔습니다.\n",
      "877번째 페이지의 내용이 들어갔습니다.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "servicekey=\"0dK1EIo6UluKLHUFpdCi0cbu2M1yQ0xDrrUwxEoEy%2Fz3ZXptbQ6rQ7KqBmuj1mTyXKglkIhWyCjs8abFAYYdDg%3D%3D\"\n",
    "j=0\n",
    "inform_list=[]\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    j+=1\n",
    "    endpoint=\"http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList?serviceKey={}&numOfRows=10&pageNo={}&dataCd=ASOS&dateCd=HR&stnIds=152&schListCnt=10&endDt=20181231&endHh=23&startHh=01&startDt=20180101\".format(servicekey,j)\n",
    "    resp=requests.get(endpoint)\n",
    "    time.sleep(3)# 찾을 시간을 줘야합니다....\n",
    "    if resp.status_code ==200 :\n",
    "        soup=BeautifulSoup(resp.text)\n",
    "        inform_=soup.find_all(\"item\")\n",
    "        for i in inform_:\n",
    "            if i.find(\"tm\").text < \"2019-01-01 00:00\":\n",
    "                inform_list.append([i.find(\"tm\").text,i.find(\"ta\").text,i.find(\"rn\").text,i.find(\"ws\").text,i.find(\"hm\").text,i.find(\"dsnw\").text])\n",
    "                print(i.find(\"tm\").text)\n",
    "                print(\" 데이터 {}개가 들어갔습니다.\".format(len(inform_list)))\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if inform_==[]:\n",
    "            break\n",
    "        \n",
    "    else: \n",
    "        break\n",
    "        \n",
    "df=pd.DataFrame(inform_list,columns=[\"시간\",\"기온\",\"강수량\",\"풍속\",\"습도\",\"적설\"])\n",
    "df.to_csv(\"ulsan_inform.csv\",index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(inform_list,columns=[\"시간\",\"기온\",\"강수량\",\"풍속\",\"습도\",\"적설\"])\n",
    "df.to_csv(\"ulsan_inform2.csv\",index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'ulsan_inform2.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1011d3fb71cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ulsan_inform2.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'ulsan_inform2.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"ulsan_inform2.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop=True\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "공유 자전거 재배치 전략 수립 연구\n",
    "\n",
    "- [ ] DB 구축 \n",
    "- [ ] 유동인구 경로 예측\n",
    "- [ ] 재배치 전략 수립\n",
    "\n",
    "\n",
    "# 0. DB 구축\n",
    "\n",
    "https://github.com/sugyeong-jo/Bicycle_sharing/blob/master/data/DB.md\n",
    "\n",
    "- [ ] 4월 13일\n",
    "\n",
    "## 0-1. pydec \n",
    "- 관련 예제 정리 \n",
    "- [ ] 4월 13일\n",
    "\n",
    "\n",
    "# 1. 유동인구 경로 예측\n",
    "\n",
    "## 1-1. 유동인구 수 예측\n",
    "- 뉴럴 네트워크를 이용한 유동인구 수 예측\n",
    "- pytorch 이용 예정\n",
    "- [ ] 4월 20일\n",
    "\n",
    "## 1-2. 유동인구 경로 예측\n",
    "- gravity spatial interaction model\n",
    "- key word: gravity spatial interaction model, OD flow, Dynamic OD estimation, ...\n",
    "- [ ] 4월 27일\n",
    "\n",
    "# 2. 경로에 따른 배터리 예측 시뮬레이션\n",
    "- 시뮬레이션 논문 찾고 구현\n",
    "- [ ] 5월 4일\n",
    "\n",
    "# 3. 재배치 전략 수립\n",
    "- Dynamic pricing model\n",
    "    > Gig economy!\n",
    "\n",
    "    > 택배\n",
    "- Vehicle routing problem \n",
    "- Equilibrium model\n",
    "- [ ] 매 중 2개 이상 찾고 정리하기\n",
    "- [ ] 5월 11일 간단한 예제 만들기\n",
    "- [ ] 5월 17일 두 가지에 대한 간단한 예제 만들기\n",
    "- [ ] 5월 24일 equilibrium model 예제 만들기\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
